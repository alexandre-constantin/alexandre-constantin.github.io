{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee408f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9994deff",
   "metadata": {},
   "source": [
    "# Notebook simulations - DOF / perceptron une couche\n",
    "\n",
    "Ce notebook présente les simulations de calcul de degrés de libertés pour des données non linéaires générées par un polynôme.\n",
    "\n",
    "Deux modèles sont présentés :\n",
    "1. Un modèle linéaire en paramètres dont la dimension d'entrée est augmentée,\n",
    "2. Un réseau de neurone (perceptron) à une couche cachée.\n",
    "\n",
    "Le notebook requiert la création d'un dossier \"Dossier_resultats\" pour fonctionner.\n",
    "Ce dernier contiendra les résultats obtenus en format csv (contient des NaN correspondants aux colonnes moins longues que d'autres, à retirer lors d'analyses).\n",
    "\n",
    "\n",
    "\n",
    "### Génération de données\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b85550",
   "metadata": {},
   "source": [
    "Génération du polynôme pour le premier cas linéaire puis pour le perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b36e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 0\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of samples\n",
    "n      = 10000          # independent samples\n",
    "n_mc   = 100           # Number of data-sets for output\n",
    "# Number of features\n",
    "d      = 2\n",
    "# Noise variance\n",
    "sigma_2= 0.1\n",
    "epsilon= np.sqrt(sigma_2)*np.random.randn(n,n_mc)\n",
    "# Regularization for both modelizations\n",
    "l1_reg = 0.1\n",
    "l2_reg = [1e-12, 1e-9, 1e-6, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 1e0, 1e1, 2e1, 5e1, 1e2, 1e3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40229eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Feature matrix\n",
    "X        = 2*np.random.random((n,d))-1\n",
    "poly     = PolynomialFeatures(5)\n",
    "X_transf = poly.fit_transform(X)\n",
    "\n",
    "# Matrix containing the observations with polynomial features\n",
    "Y        = np.tile(0.1*np.power(X[:, 0], 5) \\\n",
    "                   - 0.4*np.power(X[:,0], 2)*np.power(X[:,1], 2) \\\n",
    "                   + 0.8*np.power(X[:,0], 2)*X[:,1] \\\n",
    "                   - 0.01*np.power(X[:,1], 3) \\\n",
    "                   +0.4*X[:,1],                                    # Max order (5), then 4 / 3 / 3 / 1\n",
    "                   (n_mc, 1)).T                                    # Copy n_mc times - size n x n_mc\n",
    "Y        = Y + epsilon\n",
    "\n",
    "### Prints\n",
    "print(\"Number of features  : {}  / number of monomials : {}.\".format(X.shape[1], X_transf.shape[1]))\n",
    "print(\"Input shape (lin)   :\", X.shape)\n",
    "print(\"Input shape (nn-lin):\", X_transf.shape)\n",
    "print(\"Output shape        :\", Y[:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplot = 100\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X[:nplot,0], X[:nplot,1], Y[:nplot,0], marker=\"o\")\n",
    "ax.set_xlabel('x_1');\n",
    "ax.set_ylabel('x_2');\n",
    "ax.set_zlabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ccd83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67c90278",
   "metadata": {},
   "source": [
    "### Modèle linéaire\n",
    "\n",
    "Pour une régularisation L2, solution du problème d'optimisation :\n",
    "$$\n",
    "L(\\boldsymbol{\\theta}) = \\lVert \\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y} \\rVert^2_2 + \\eta \\lVert \\boldsymbol{\\theta} \\rVert^2_2.\n",
    "$$\n",
    "La solution est :\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^\\top\\mathbf{X} + \\eta\\mathbf{I})^{-1}\\mathbf{X}^\\top \\mathbf{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887afaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data-set between 1 and n_mc\n",
    "ds = 0\n",
    "# Save dofs & MSE\n",
    "dfs = []\n",
    "mses = []\n",
    "# Optimize model with L2 penalty\n",
    "for eta in l2_reg:\n",
    "    inv_ = np.linalg.inv(np.dot(X_transf.T, X_transf) + eta*np.eye(X_transf.shape[1]))\n",
    "    hat_theta = np.dot(inv_, np.dot(X_transf.T, Y[:, ds:(ds+1)]))\n",
    "                \n",
    "    # Compute outputs\n",
    "    hat_y = np.dot(X_transf, hat_theta)\n",
    "    \n",
    "    # Compute MSE\n",
    "    mses.append(MSE(Y[:,ds], hat_y))\n",
    "    # Compute dofs\n",
    "    H = np.dot(X_transf.T, X_transf)     # Hessian without penalty\n",
    "    vp, _ = np.linalg.eig(H)             # get eignevalues\n",
    "    df_lin = np.sum(np.divide(vp, vp + eta))\n",
    "    dfs.append(df_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e660e2e",
   "metadata": {},
   "source": [
    "Calcul des degrés de liberté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(l2_reg, dfs, label=\"Degrees of freedom\")\n",
    "plt.plot(l2_reg, hat_theta.shape[0]*np.ones(len(l2_reg)), 'r', label=\"Nb params\")\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(l2_reg, mses, label=\"MSE\")\n",
    "plt.plot(l2_reg, sigma_2*np.ones(len(l2_reg)), 'r', label=\"Noise variance\")\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "\n",
    "# Save figures to csv\n",
    "dict_graph = {}\n",
    "\n",
    "dict_graph[\"l2_eta\"]    = np.array(l2_reg)\n",
    "dict_graph[\"dofs\"]      = np.array(dfs)\n",
    "dict_graph[\"MSE\"]       = np.array(mses)\n",
    "dict_graph[\"var\"]       = np.array(sigma_2*np.ones(len(l2_reg)))\n",
    "dict_graph[\"nb_params\"] = np.array(hat_theta.shape[0]*np.ones(len(l2_reg)))\n",
    "\n",
    "df = pd.DataFrame.from_dict(dict_graph, orient = \"index\")\n",
    "df = df.T\n",
    "df.to_csv(\"Dossier_resultats/lin_poly_degree_5_L2.csv\",\n",
    "          na_rep=\"nan\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e708575",
   "metadata": {},
   "source": [
    "#### Calcul du critère AIC pour différents ordres de polynômes sans régularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "ds  = 0 # Select data-set between 1 and n_mc\n",
    "eta = 0 # no regularization\n",
    "# Save data\n",
    "aic_poly = [];  bic_poly = [];\n",
    "mse_poly = []; feat_poly = [];\n",
    "\n",
    "# Compute for order 1\n",
    "inv_ = np.linalg.inv(np.dot(X.T, X) + eta*np.eye(X.shape[1]))\n",
    "hat_theta = np.dot(inv_, np.dot(X.T, Y[:, ds:(ds+1)]))\n",
    "hat_y = np.dot(X, hat_theta)\n",
    "aic_poly.append(n*np.log(MSE(Y[:,0], hat_y)) + 2*X.shape[1])\n",
    "bic_poly.append(n*np.log(MSE(Y[:,0], hat_y)) + np.log(n)*X.shape[1])\n",
    "mse_poly.append(MSE(Y[:,0], hat_y))\n",
    "feat_poly.append(X.shape[1])\n",
    "print(\"Degree {} : {} features\".format(1, X.shape[1]))\n",
    "\n",
    "# Compute for higher order\n",
    "for deg in degrees[1:]:\n",
    "    poly_tmp = PolynomialFeatures(deg)\n",
    "    X_tmp    = poly_tmp.fit_transform(X)\n",
    "    # Solve opt.\n",
    "    inv_ = np.linalg.inv(np.dot(X_tmp.T, X_tmp) + eta*np.eye(X_tmp.shape[1]))\n",
    "    hat_theta = np.dot(inv_, np.dot(X_tmp.T, Y[:, ds:(ds+1)]))\n",
    "    hat_y = np.dot(X_tmp, hat_theta)\n",
    "    aic_poly.append(n*np.log(MSE(Y[:,0], hat_y)) + 2*X_tmp.shape[1])\n",
    "    bic_poly.append(n*np.log(MSE(Y[:,0], hat_y)) + np.log(n)*X_tmp.shape[1])\n",
    "    mse_poly.append(MSE(Y[:,0], hat_y))\n",
    "    \n",
    "    feat_poly.append(X_tmp.shape[1])\n",
    "    print(\"Degree {} : {} features\".format(deg, X_tmp.shape[1]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd58fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(degrees, aic_poly, label=\"AIC\")\n",
    "plt.plot(degrees, bic_poly, 'r', label=\"BIC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(degrees, mse_poly, label=\"MSE\")\n",
    "plt.xlabel(\"Degrees\")\n",
    "plt.legend()\n",
    "\n",
    "# Save figures to csv\n",
    "dict_graph = {}\n",
    "\n",
    "dict_graph[\"degrees\"]  = np.array(degrees)\n",
    "dict_graph[\"AIC\"]      = np.array(aic_poly)\n",
    "dict_graph[\"BIC\"]      = np.array(bic_poly)\n",
    "dict_graph[\"MSE\"]      = np.array(mse_poly)\n",
    "dict_graph[\"features\"] = np.array(feat_poly)\n",
    "\n",
    "df = pd.DataFrame.from_dict(dict_graph, orient = \"index\")\n",
    "df = df.T\n",
    "df.to_csv(\"Dossier_resultats/lin_poly_degrees_AIC.csv\",\n",
    "          na_rep=\"nan\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc723da",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "Imports - Utilisation de Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2,l1\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "from scipy.linalg import cho_factor, cho_solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b18ff14",
   "metadata": {},
   "source": [
    "Définition des fonctions pour le calcul des dofs (hessienne et gradient) pour une somme des erreurs quadratiques (SEQ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ec079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):    # Activation function\n",
    "    return np.tanh(x)\n",
    "def dsig(x):   # 1st Derivative of act. func.\n",
    "    return np.ones_like(x) - np.power(sig(x), 2)\n",
    "def ddsig(x):  # 2nd Derivative of act. func.\n",
    "    return -2*sig(x)*dsig(x)\n",
    "\n",
    "def hessian_i(theta, X, y):\n",
    "    # Check if one sample is given\n",
    "    if X.shape[0] > 1:\n",
    "        print(\"Give one sample to the function with\")\n",
    "        print(\" shape (1, p),\")\n",
    "        print(\" where p is the number of features.\")\n",
    "        X = X[0:1, :]\n",
    "        y = y[0].reshape(-1, 1)\n",
    "    # Compute gradient\n",
    "    grady = grady_i(theta, X, y)\n",
    "    # Get parameters\n",
    "    W1 = theta[0];       b1 = theta[1];\n",
    "    Wo = theta[2][:, 0]; bo = theta[3];\n",
    "    # Compute number of parameters and initialize\n",
    "    lw1 = len(W1.ravel());  lb1 = len(b1);\n",
    "    lwo = len(Wo.ravel());  lbo = len(bo);\n",
    "    d = W1.shape[1]    # Number of hidden layers\n",
    "    nb_params = lw1 + lb1 + lwo + lbo;\n",
    "    Hy = np.zeros((nb_params, nb_params))\n",
    "\n",
    "    # Compute outputs of layers\n",
    "    O1 = np.matmul(W1.T, X[0, :]) + b1\n",
    "    yHat = np.dot(Wo, sig(O1)) + bo\n",
    "\n",
    "    # Compute Hessian of output wrt params\n",
    "    Hy[0:lw1, 0:lw1] = \\\n",
    "        np.kron(np.diag(ddsig(O1)*Wo),\n",
    "                np.matmul(X.T, X))          # W1 / W1\n",
    "    Hy[0:lw1, lw1:(lw1+lwo)] = \\\n",
    "        np.kron(np.diag(dsig(O1)), X).T     # W1 / Wo\n",
    "    Hy[lw1:(lw1+lwo), 0:lw1] = Hy[0:lw1, lw1:(lw1+lwo)].T\n",
    "    Hy[0:lw1, (lw1+lwo):(lw1+lwo+lb1)] = \\\n",
    "        np.kron(np.diag(ddsig(O1)*Wo), X).T # W1 / b1\n",
    "    Hy[(lw1+lwo):(lw1+lwo+lb1), 0:lw1] = Hy[0:lw1, (lw1+lwo):(lw1+lwo+lb1)].T\n",
    "    Hy[lw1:(lw1+lwo), (lw1+lwo):(lw1+lwo+lb1)] = \\\n",
    "        np.diag(dsig(O1))                   # Wo / b1\n",
    "    Hy[(lw1+lwo):(lw1+lwo+lb1), lw1:(lw1+lwo)] = Hy[lw1:(lw1+lwo), (lw1+lwo):(lw1+lwo+lb1)].T\n",
    "    Hy[(lw1+lwo):(lw1+lwo+lb1), (lw1+lwo):(lw1+lwo+lb1)] = \\\n",
    "        np.diag(ddsig(O1)*Wo)               # b1 / b1\n",
    "\n",
    "    return (yHat - y)*Hy + np.matmul(grady, grady.T)  # SSE formula\n",
    "\n",
    "def grady_i(theta, X, y):\n",
    "    # Check if one sample is given\n",
    "    if X.shape[0] > 1:\n",
    "        print(\"Give one sample to the function with\")\n",
    "        print(\" shape (1, p),\")\n",
    "        print(\" where p is the number of features.\")\n",
    "        X = X[0:1, :]\n",
    "        y = y[0].reshape(-1, 1)\n",
    "    # Get parameters\n",
    "    W1 = theta[0];       b1 = theta[1];\n",
    "    Wo = theta[2][:, 0]; bo = theta[3];\n",
    "    # Compute number of parameters and initialize\n",
    "    lw1 = len(W1.ravel());  lb1 = len(b1);\n",
    "    lwo = len(Wo.ravel());  lbo = len(bo);\n",
    "    d = W1.shape[1]    # Number of hidden layers\n",
    "    nb_params = lw1 + lb1 + lwo + lbo;\n",
    "    grady = np.zeros((nb_params)).reshape(-1, 1)\n",
    "    \n",
    "    O1 = np.matmul(W1.T, X[0, :]) + b1\n",
    "    # Compute gradient of output wrt params\n",
    "    grady[0:lw1] = np.kron(dsig(O1)*Wo, X).T      # W1\n",
    "    grady[lw1:(lw1+lwo)] = sig(O1).reshape(-1, 1) # Wo\n",
    "    grady[(lw1+lwo):(lw1+lwo+lb1)] = (dsig(O1)*Wo).reshape(-1,1) # b1\n",
    "    grady[(lw1+lwo+lb1):(lw1+lwo+lb1+lbo)] = 1    # bo\n",
    "\n",
    "    return grady"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999e7b9",
   "metadata": {},
   "source": [
    "#### Model settings\n",
    "\n",
    "Définition du modèle pour un choix de $\\eta$ et un choix de $m$ (nombre de neurones dans la couche cachée).\n",
    "Calculera les dof avec la méthode de Ye (et enregistre) pour cette configuration.\n",
    "\n",
    "L'ensemble des résultats, du document produit, nécessitent de répéter cette étape pour différentes tailles et différentes régularisation.\n",
    "\n",
    "A souligner que les différents tests de l'estimateur proposés sont réalisés plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "# Number of hidden layers of the perceptron model\n",
    "k      = 1\n",
    "# Number of neurons per hidden layer\n",
    "m      = 2\n",
    "# Reg number or position in l2_reg\n",
    "eta    = 1e-12\n",
    "# Learning settings\n",
    "initial_learning_rate = 1.\n",
    "final_learning_rate = 0.01\n",
    "ep      = 600\n",
    "t_size  = n\n",
    "b_size  = 100\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "for i in range(0,k):\n",
    "    model.add(Dense(m, activation='tanh',kernel_initializer='he_normal',kernel_regularizer=l2(eta)))\n",
    "model.add(Dense(1, activation='linear',kernel_regularizer=l2(eta)))\n",
    "\n",
    "# Learning settings\n",
    "learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/ep)\n",
    "steps_per_epoch = int(t_size/b_size)\n",
    "\n",
    "lr_schedule =   ExponentialDecay(\n",
    "                initial_learning_rate=initial_learning_rate,\n",
    "                decay_steps=steps_per_epoch,\n",
    "                decay_rate=learning_rate_decay_factor,\n",
    "                staircase=True)\n",
    "# Compile the model\n",
    "sgd = SGD(learning_rate=lr_schedule)\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "model.build(X.shape)\n",
    "model.summary()\n",
    "# Save weights\n",
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60733411",
   "metadata": {},
   "source": [
    "Test sur un jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dce6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init variables\n",
    "mses_per = []\n",
    "lambda_ = 1e-12\n",
    "# Fit the model\n",
    "for i in range(ep):\n",
    "    history = model.fit(X, Y[:,0], epochs=1, batch_size=b_size, verbose=0)\n",
    "    if i < 10 or (i+1) % 100 == 0:\n",
    "        print(\"Epoch {} / {}  -  Loss: {:.4f} / mse: {:.4f}\".format(\n",
    "            i+1, ep, history.history['loss'][0], history.history['mse'][0]))\n",
    "# Evaluate predictions\n",
    "Y_pred = model.predict(X)\n",
    "# Compute dofs\n",
    "theta = [v.numpy() for v in model.trainable_variables]\n",
    "nb_params = sum([len(v.ravel()) for v in theta])\n",
    "# compute Hessian and Jacobians\n",
    "H = np.zeros((nb_params, nb_params))\n",
    "J_grad = np.zeros((nb_params, n))\n",
    "Jy_m = np.zeros((n, nb_params))\n",
    "# Loop on samples\n",
    "for i in range(0, X.shape[0]):\n",
    "    H += hessian_i(theta, X[i:i+1, :], Y[i, 0])\n",
    "    Jy_m[i, :] = grady_i(theta, X[i:i+1, :], Y[i, 0]).T\n",
    "    J_grad[:, i:(i+1)] = grady_i(theta, X[i:i+1, :], Y[i, 0])\n",
    "\n",
    "# Normalize H (Frobenius norm)\n",
    "froe_H = np.sqrt(np.trace(H.T.dot(H)))\n",
    "H /= froe_H\n",
    "J_grad /= -froe_H\n",
    "# Compute the matrix to put within the trace as linear problem\n",
    "H_inv_J_grad = np.linalg.lstsq(H + lambda_*np.eye(nb_params), J_grad, rcond=-1)[0] # rcond=None\n",
    "Jy_y = -np.matmul(Jy_m, H_inv_J_grad)\n",
    "# Compute dof - general formula with Gaussian iid data + Save MSE\n",
    "df_ours.append(np.trace(Jy_y))\n",
    "mses_per.append(MSE(Y_pred, Y[:, 0]))\n",
    "\n",
    "print(\"\\nDegrees of freedom : {} / Nb of parameters : {}\".format(df_ours[0], nb_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b778d",
   "metadata": {},
   "source": [
    "Algorithm de Ye pour $\\sigma$ connu et enregistré par sa vraie valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12256e53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hatMu = np.zeros((n*n_mc, 1))\n",
    "delta_mat = np.zeros((n*n_mc, n+1))\n",
    "tau = 0.6*np.sqrt(sigma_2);              # std_deviation suggested by Ye, 1998 - \\sigma is known\n",
    "delta_mat[:, n] = 1\n",
    "\n",
    "for i in range(n_mc):\n",
    "    # Reinitialize model\n",
    "    model.set_weights(weights)\n",
    "    print('Monte-Carlo realization i: ',i)\n",
    "    # Data perturbation\n",
    "    delta_t = tau*np.random.randn(n).reshape(-1, 1)\n",
    "    y_train_per = Y[:, 0].reshape(-1,1) + delta_t\n",
    "    # Fit the model\n",
    "    model.fit(X, y_train_per, epochs=ep,\n",
    "              batch_size=b_size, verbose=0)\n",
    "    # Evaluate predictions\n",
    "    Y_pred = model.predict(X)\n",
    "    # Save results for Ye\n",
    "    hatMu[i::n_mc, :] = Y_pred            # See Ye, 1998\n",
    "    delta_mat[i::n_mc, :n] = np.diag(delta_t[:,0])\n",
    "    \n",
    "    # Save MSE\n",
    "    mses_per.append(MSE(Y_pred, Y[:, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3185c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mat_sub = np.empty((n_mc, 2))\n",
    "delta_mat_sub[:, 1] = 1;\n",
    "# Ye solution\n",
    "dof_Ye = 0\n",
    "\n",
    "for k in range(0, n):\n",
    "    # Fill the matrices\n",
    "    delta_mat_sub[:, 0] = delta_mat[k*n_mc:(k+1)*n_mc, k]\n",
    "    hatMu_sub = hatMu[k*n_mc:(k+1)*n_mc, :]\n",
    "    # Compute dof\n",
    "    sol_ = np.linalg.solve(delta_mat_sub.T.dot(delta_mat_sub) + lambda_*np.eye(2),\n",
    "                           delta_mat_sub.T.dot(hatMu_sub))\n",
    "    # Compute dof as the sum of the slopes\n",
    "    dof_Ye += sol_[0]\n",
    "    \n",
    "# Print\n",
    "print(dof_Ye[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87946282",
   "metadata": {},
   "source": [
    "#### Calcul de la convergence de l'algorithme Ye (étape par étape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290988b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int(n/2))\n",
    "print(int(n/100))\n",
    "print(int(n/50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6fc04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dof_MC = np.zeros((n_mc))\n",
    "\n",
    "for t in range(1, n_mc+1):\n",
    "    # Extract indexes\n",
    "    hatMu_sub = np.empty((t, 1))\n",
    "    delta_mat_sub = np.empty((t, 2))\n",
    "    delta_mat_sub[:, 1] = 1;\n",
    "    \n",
    "    # Loop\n",
    "    for k in range(0, n):\n",
    "        delta_mat_sub[:, 0] = delta_mat[k*n_mc:(k*n_mc+t), k]\n",
    "        hatMu_sub = hatMu[k*n_mc:(k*n_mc+t), :]\n",
    "        # Compute dof\n",
    "        sol_ = np.linalg.solve(delta_mat_sub.T.dot(delta_mat_sub) + lambda_*np.eye(2),\n",
    "                               delta_mat_sub.T.dot(hatMu_sub))\n",
    "        dof_MC[t-1] += sol_[0]\n",
    "    \n",
    "    # print\n",
    "    if t%20 == 0:\n",
    "        print(\"Etape {} ok.\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebe904",
   "metadata": {},
   "source": [
    "Affichage de la convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(1, n_mc+1), dof_MC)\n",
    "plt.title(\"Convergence algo Ye\")\n",
    "plt.ylim(-5, 30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f7fe4",
   "metadata": {},
   "source": [
    "Et affichage de l'erreur quadratique de chaque run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b88c04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(list(range(1, n_mc+1)), mses_per[1:])\n",
    "plt.title(\"MSE\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a1407",
   "metadata": {},
   "source": [
    "Sauvegarde dans .csv (dépend du nombre de neurones - voir format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1372ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_graph = {}\n",
    "\n",
    "dict_graph[\"simus\"]    = np.arange(1, n_mc+1)\n",
    "dict_graph[\"conv_Ye\"]  = dof_MC\n",
    "dict_graph[\"dof\"]      = np.array(df_ours[1:])   # Remove the first one (previous simulation)\n",
    "dict_graph[\"MSE\"]      = np.array(mses_per)\n",
    "\n",
    "df = pd.DataFrame.from_dict(dict_graph, orient = \"index\")\n",
    "df = df.T\n",
    "df.to_csv(\"Dossier_resultats/MLP_results_nbHNeurons{}_reg{}.csv\".format(m, int(-np.log10(eta))),\n",
    "          na_rep=\"nan\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d972a9",
   "metadata": {},
   "source": [
    "#### Calcul de notre estimateur pour différentes architectures\n",
    "\n",
    "Boucle pour parcourir plusieurs dimensions et renvoyer les degrés de liberté.\n",
    "$\\eta$ est parcouru parmi \"etas\" (liste) et le nombre de neurones de la couche cachée varient entre 1 et \"n_max\".\n",
    "\n",
    "En fin de boucle les résultats sont sauvegardés dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914bc029",
   "metadata": {},
   "outputs": [],
   "source": [
    "etas   = [1e-2, 1e-3, 1e-6, 1e-12]\n",
    "m_max  = 10             # Maximum number of hidden neurons\n",
    "nb_params_mlp = []         # Save number of parameters\n",
    "\n",
    "# Number of hidden layers of the perceptron model\n",
    "k      = 1\n",
    "# Learning settings\n",
    "initial_learning_rate = 1.\n",
    "final_learning_rate = 0.01\n",
    "ep      = 600\n",
    "t_size  = n\n",
    "b_size  = 10\n",
    "learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/ep)\n",
    "steps_per_epoch = int(t_size/b_size)\n",
    "\n",
    "# Save dict\n",
    "dict_graph = {}\n",
    "# Loops\n",
    "for eta in etas:       # Reg number or position in l2_reg\n",
    "    mses = [];\n",
    "    hat_df = [];\n",
    "    gcvs =[];\n",
    "    \n",
    "    for m in range(1, m_max+1):   # For different neurons size\n",
    "        set_seed(seed)  # reset seed\n",
    "\n",
    "        # Define model\n",
    "        model = Sequential()\n",
    "        for i in range(0,k):\n",
    "            model.add(Dense(m, activation='tanh',kernel_initializer='he_normal',kernel_regularizer=l2(eta)))\n",
    "            model.add(Dense(1, activation='linear',kernel_regularizer=l2(eta)))\n",
    "\n",
    "            lr_schedule =   ExponentialDecay(\n",
    "                        initial_learning_rate=initial_learning_rate,\n",
    "                        decay_steps=steps_per_epoch,\n",
    "                        decay_rate=learning_rate_decay_factor,\n",
    "                        staircase=True)\n",
    "        # Compile the model\n",
    "        sgd = SGD(learning_rate=lr_schedule)\n",
    "        model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "        model.build(X.shape)\n",
    "        # model.summary()\n",
    "        \n",
    "        # Compute dofs\n",
    "        lambda_ = 1e-10        # Regularization for inversion\n",
    "        # Fit the model\n",
    "        for i in range(ep):\n",
    "            history = model.fit(X, Y[:,0], epochs=1, batch_size=b_size, verbose=0)\n",
    "            if (i+1) == 500 or (i+1) == 600:\n",
    "                print(\"Epoch {} / {}  -  Loss: {:.4f} / mse: {:.4f}\".format(\n",
    "                      i+1, ep, history.history['loss'][0], history.history['mse'][0]))\n",
    "        # Evaluate predictions\n",
    "        Y_pred = model.predict(X)\n",
    "        # Compute dofs\n",
    "        theta = [v.numpy() for v in model.trainable_variables]\n",
    "        nb_params = sum([len(v.ravel()) for v in theta])\n",
    "        # compute Hessian and Jacobians\n",
    "        H = np.zeros((nb_params, nb_params))\n",
    "        J_grad = np.zeros((nb_params, n))\n",
    "        Jy_m = np.zeros((n, nb_params))\n",
    "        # Loop on samples\n",
    "        for i in range(0, X.shape[0]):\n",
    "            H += hessian_i(theta, X[i:i+1, :], Y[i, 0])\n",
    "            Jy_m[i:i+1, :] = grady_i(theta, X[i:i+1, :], Y[i, 0]).T\n",
    "            J_grad[:, i:i+1] = grady_i(theta, X[i:i+1, :], Y[i, 0])\n",
    "        # Normalize H (Frobenius norm)\n",
    "        froe_H = np.sqrt(np.trace(H.T.dot(H)))\n",
    "        H /= froe_H\n",
    "        J_grad /= -froe_H\n",
    "        # Compute the matrix to put within the trace as linear problem\n",
    "        H_inv_J_grad = np.linalg.lstsq(H + lambda_*np.eye(nb_params), J_grad, rcond=None)[0]\n",
    "        Jy_y = -np.matmul(Jy_m, H_inv_J_grad)\n",
    "        # Compute dof - general formula with Gaussian iid data + Save MSE\n",
    "        h_df = np.trace(Jy_y); mse_r = MSE(Y_pred, Y[:, 0]);\n",
    "        hat_df.append(h_df)\n",
    "        mses.append(mse_r)\n",
    "        gcvs.append(n*mse_r/((n-h_df)*(n-h_df)))\n",
    "        \n",
    "        if eta == etas[0]:\n",
    "            nb_params_mlp.append(nb_params)\n",
    "            \n",
    "        print(\"eta - {},  nb_neurons - {},  Done.\".format(int(-np.log10(eta)), m))\n",
    "            \n",
    "    dict_graph[\"MSE_eps{}\".format(int(-np.log10(eta)))] = np.array(mses)\n",
    "    dict_graph[\"df_eps{}\".format(int(-np.log10(eta)))]  = np.array(hat_df)\n",
    "    dict_graph[\"GCV_eps{}\".format(int(-np.log10(eta)))]  = np.array(gcvs)\n",
    "\n",
    "dict_graph[\"nb_neurons\"] = np.arange(1, m_max+1)\n",
    "dict_graph[\"nb_params\"]  = np.array(nb_params_mlp)\n",
    "\n",
    "df = pd.DataFrame.from_dict(dict_graph, orient = \"index\")\n",
    "df = df.T\n",
    "df.to_csv(\"Dossier_resultats/MLP_results_hat_df.csv\",\n",
    "          na_rep=\"nan\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3e509",
   "metadata": {},
   "source": [
    "Fin de document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
